{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM-GNN Rollout Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HeteroConv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import json \n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from torch_geometric.nn.models import MLP\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "np.set_printoptions(linewidth=200)\n",
    "torch.set_printoptions(linewidth=200)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_functions import HeteroDEMDataset\n",
    "dataset = HeteroDEMDataset('2Sphere',force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('particle',\n",
       "  'PP_contact',\n",
       "  'particle'): tensor([[ 2.5001e-07,  3.9436e-08,  6.5345e-01,  6.5345e-01],\n",
       "         [-2.5001e-07, -3.9436e-08, -6.5345e-01,  6.5345e-01]], dtype=torch.float64),\n",
       " ('particle',\n",
       "  'PW_contact',\n",
       "  'wallpoint'): tensor([[-0.3277,  0.0000,  0.0000,  0.3277],\n",
       "         [ 0.0000, -0.3277,  0.0000,  0.3277],\n",
       "         [ 0.0000, -0.3277,  0.0000,  0.3277],\n",
       "         [ 0.0000,  0.0000, -0.3267,  0.3267],\n",
       "         [ 0.3277,  0.0000,  0.0000,  0.3277],\n",
       "         [ 0.0000,  0.0000,  0.3267,  0.3267]], dtype=torch.float64),\n",
       " ('wallpoint',\n",
       "  'rev_PW_contact',\n",
       "  'particle'): tensor([[ 0.3277,  0.0000,  0.0000,  0.3277],\n",
       "         [ 0.0000,  0.3277,  0.0000,  0.3277],\n",
       "         [ 0.0000,  0.3277,  0.0000,  0.3277],\n",
       "         [ 0.0000,  0.0000,  0.3267,  0.3267],\n",
       "         [-0.3277,  0.0000,  0.0000,  0.3277],\n",
       "         [ 0.0000,  0.0000, -0.3267,  0.3267]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,0,1])\n",
    "d = np.array([1,1,1])\n",
    "d-(d*a)*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "class EdgeConv(MessagePassing):\n",
    "    def __init__(self,emb_dim,hidden_dim,num_layers, aggr = 'mean', ):\n",
    "        super().__init__(aggr=aggr)\n",
    "        self.msg_mlp = MLP(in_channels=8+2*emb_dim,hidden_channels=hidden_dim,out_channels=hidden_dim,num_layers=num_layers)\n",
    "        self.node_mlp = MLP(in_channels=hidden_dim+emb_dim,hidden_channels=hidden_dim,out_channels=emb_dim,num_layers=num_layers)\n",
    "        self.edge_mlp = MLP(in_channels=hidden_dim,hidden_channels=hidden_dim,out_channels=emb_dim,num_layers=num_layers)\n",
    "\n",
    "\n",
    "    def forward(self,x,edge_attr,edge_index):\n",
    "        node_emb = self.propagate(edge_index,x=x,edge_attr=edge_attr)\n",
    "        edge_emb = self.edge_updater(edge_index,x=x,edge_attr=edge_attr)\n",
    "        return node_emb, edge_emb\n",
    "    \n",
    "    def message(self, x_j, edge_attr):\n",
    "        tmp = torch.cat([x_j, edge_attr],dim=1)\n",
    "        return self.msg_mlp(tmp)\n",
    "    \n",
    "    def update(self,aggr_out,x_i):\n",
    "        if True:\n",
    "            if isinstance(x_i,tuple):\n",
    "                print(f'tuple: {[i.shape for i in x_i]}')\n",
    "            else:\n",
    "                print(f'array: {x_i.shape}')\n",
    "            if isinstance(aggr_out,tuple):\n",
    "                print(f'tuple: {[i.shape for i in aggr_out]}')   \n",
    "            else:\n",
    "                print(f'array: {aggr_out.shape}')\n",
    "        cat = torch.cat([x_i, aggr_out],dim=-1)\n",
    "        return self.node_mlp(cat) \n",
    "    \n",
    "    def edge_update(self, x_j, edge_attr):\n",
    "        message = self.message(x_j,edge_attr)\n",
    "        return self.edge_mlp(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDEMGNN(torch.nn.Module):\n",
    "    def __init__(self,metadata, msg_num,emb_dim,hidden_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.nodetypes = metadata[0]\n",
    "        self.edgetypes = metadata[1]\n",
    "\n",
    "        self.edge_embed_mlp = MLP(in_channels=4,hidden_channels=emb_dim,out_channels=emb_dim,num_layers=num_layers)\n",
    "        self.node_embed_mlp = torch.nn.ModuleDict()\n",
    "        for nodetype in metadata[0]:\n",
    "            embed_mlp =MLP(in_channels=3,hidden_channels=emb_dim,out_channels=emb_dim,num_layers=num_layers)\n",
    "            self.node_embed_mlp[nodetype] = embed_mlp\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.decoders = torch.nn.ModuleList()\n",
    "        for _ in range(msg_num):\n",
    "            conv = HeteroConv({edgetype:EdgeConv(emb_dim,hidden_dim,num_layers)\n",
    "                               for edgetype in self.edgetypes},aggr=\"mean\")\n",
    "            decoder = MLP(in_channels=emb_dim,hidden_channels=hidden_dim,out_channels=3,num_layers=num_layers)\n",
    "            self.convs.append(conv)\n",
    "            self.decoders.append(decoder)\n",
    "        self.double()\n",
    "\n",
    "    def EmbedNodes(self,x_dict:dict) -> dict:\n",
    "        node_emb_dict = {}\n",
    "        for nodetype in self.nodetypes:\n",
    "            node_emb_dict[nodetype] = self.node_embed_mlp[nodetype](x_dict[nodetype])\n",
    "        return node_emb_dict\n",
    "    \n",
    "    def EmbedEdges(self,edge_attr_dict):\n",
    "        edge_emb_dict = {}\n",
    "        for edgetype in self.edgetypes:\n",
    "            edge_emb_dict[edgetype] = self.edge_embed_mlp(edge_attr_dict[edgetype])\n",
    "        return edge_emb_dict\n",
    "    \n",
    "    def MergeEdgeDicts(self,edge_ref_dict,edge_attr_dict,edge_emb_dict):\n",
    "        edge_inp_dict = {}\n",
    "        for edgetype in self.edgetypes:\n",
    "            tensors = (edge_ref_dict[edgetype],edge_attr_dict[edgetype],edge_emb_dict[edgetype])\n",
    "            edge_inp_dict[edgetype] = torch.cat(tensors,dim=1)\n",
    "        return edge_inp_dict\n",
    "\n",
    "    def forward(self,data):\n",
    "        x_dict, edge_attr_dict, edge_index_dict = data.x_dict, data.edge_attr_dict, data.edge_index_dict\n",
    "        edge_ref_dict = edge_attr_dict.copy()\n",
    "\n",
    "        x_dict = self.EmbedNodes(x_dict)\n",
    "        edge_emb_dict = self.EmbedEdges(edge_attr_dict)\n",
    "\n",
    "        for conv,decoder in zip(self.convs,self.decoders):\n",
    "            edge_inp_dict = self.MergeEdgeDicts(edge_ref_dict,edge_attr_dict,edge_emb_dict)\n",
    "            print([edge_inp_dict[edgetype].shape for edgetype in self.edgetypes])\n",
    "            print([x_dict[nodetype].shape for nodetype in self.nodetypes])\n",
    "            x_dict, edge_emb_dict = conv(x_dict,edge_inp_dict,edge_index_dict)\n",
    "            displacements += decoder(x_dict['particles'])\n",
    "            print('Loop completed')\n",
    "        \n",
    "        return displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 40]), torch.Size([6, 40]), torch.Size([6, 40])]\n",
      "[torch.Size([2, 32]), torch.Size([6, 32])]\n",
      "array: torch.Size([2, 32])\n",
      "array: torch.Size([2, 32])\n",
      "array: torch.Size([6, 32])\n",
      "array: torch.Size([6, 32])\n",
      "array: torch.Size([6, 32])\n",
      "array: torch.Size([2, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 6 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m HeteroDEMGNN(data\u001b[38;5;241m.\u001b[39mmetadata(),msg_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,emb_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m displacements \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[89], line 53\u001b[0m, in \u001b[0;36mHeteroDEMGNN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m([edge_inp_dict[edgetype]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m edgetype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medgetypes])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m([x_dict[nodetype]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m nodetype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodetypes])\n\u001b[1;32m---> 53\u001b[0m x_dict, edge_emb_dict \u001b[38;5;241m=\u001b[39m conv(x_dict,edge_inp_dict,edge_index_dict)\n\u001b[0;32m     54\u001b[0m displacements \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m decoder(x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticles\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoop completed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch_geometric\\nn\\conv\\hetero_conv.py:158\u001b[0m, in \u001b[0;36mHeteroConv.forward\u001b[1;34m(self, *args_dict, **kwargs_dict)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_edge_level_arg:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m out \u001b[38;5;241m=\u001b[39m conv(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dst \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out_dict:\n\u001b[0;32m    161\u001b[0m     out_dict[dst] \u001b[38;5;241m=\u001b[39m [out]\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[88], line 11\u001b[0m, in \u001b[0;36mEdgeConv.forward\u001b[1;34m(self, x, edge_attr, edge_index)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,edge_attr,edge_index):\n\u001b[1;32m---> 11\u001b[0m     node_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index,x\u001b[38;5;241m=\u001b[39mx,edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[0;32m     12\u001b[0m     edge_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index,x\u001b[38;5;241m=\u001b[39mx,edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node_emb, edge_emb\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\DEM-GNN\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:550\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m         out \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m    548\u001b[0m update_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mcollect_param_data(\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m--> 550\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mupdate_kwargs)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decomposed_layers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    553\u001b[0m     decomp_out\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "Cell \u001b[1;32mIn[88], line 29\u001b[0m, in \u001b[0;36mEdgeConv.update\u001b[1;34m(self, aggr_out, x_i)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maggr_out\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_i, aggr_out],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_mlp(cat)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 6 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model = HeteroDEMGNN(data.metadata(),msg_num=3,emb_dim=32,hidden_dim=32,num_layers=2)\n",
    "displacements = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('particle', 'PP_contact', 'particle'),\n",
       " ('particle', 'PW_contact', 'wallpoint'),\n",
       " ('wallpoint', 'rev_PW_contact', 'particle')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "data['particle'].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edgetype in data.metadata()[1]:\n",
    "    print(*edgetype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "pre_transform = T.Compose([T.ToUndirected(),T.Cartesian(False),T.Distance(norm=False,cat=True)])\n",
    "data = pre_transform(data['particle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['particle'].y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import AggregateRawData,GetDataDir\n",
    "dataset_name = '2Sphere'\n",
    "Aggregate = AggregateRawData(dataset_name,GetDataDir())\n",
    "print(f\"Runtime {dataset_name}: {Aggregate.RuntimeAnalysis()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_functions import DEM_Dataset\n",
    "dataset_name    = \"N400_Mono\"\n",
    "[dataset_train]      = [DEM_Dataset(dataset_name, dataset_type, force_reload=False, bundle_size=1) for dataset_type in [\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetExperimentSettings,EvaluateExperiment\n",
    "exp_settings = GetExperimentSettings('N400layer','N400_Mono',push=True)\n",
    "metric_dict = EvaluateExperiment(exp_settings,save_name='N400layer_Push',batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetExperimentSettings,EvaluateExperiment\n",
    "exp_settings = GetExperimentSettings('2Sphere','2Sphere',push=True)\n",
    "metric_dict = EvaluateExperiment(exp_settings,save_name='2Sphere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotTrainingLoss\n",
    "dataset_name = '2Sphere'\n",
    "model_ident = 'lr_medium'\n",
    "fig,axs = PlotTrainingLoss(dataset_name,model_ident,push=True)\n",
    "\n",
    "fig.suptitle(f\"Model: {model_ident}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotXYZ, PlotStressComparison, legend_without_duplicate_labels\n",
    "from ML_functions import LearnedSimulator, NormalizeData, GetModel, Rescale, NormalizePos, MaskTestData\n",
    "from Encoding import SaveRolloutAsJSON\n",
    "from Evaluation import ParticlesOutsideBoundary\n",
    "\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"Allout_Push\"\n",
    "\n",
    "model = GetModel(dataset_name,model_ident)[0]\n",
    "AggregatedArgs = MaskTestData(dataset_name,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_name = f\"{dataset_name}_bund{model.bundle_size}\"\n",
    "transform = T.Compose([T.Cartesian(False),T.Distance(norm=False,cat=True),NormalizeData(dataset_name,scale_name)])\n",
    "Simulation = LearnedSimulator(model, scale_function = Rescale(dataset_name,scale_name),transform = transform)\n",
    "\n",
    "if dataset_name == '2Sphere': plot=True\n",
    "plot=False\n",
    "\n",
    "if plot==True: fig, axs = plt.subplots(1,3,sharey=True,figsize=(15,5))\n",
    "for sample_idx in trange(1):\n",
    "    Simulation.Rollout(*AggregatedArgs,sample_idx,show_tqdm=False)\n",
    "    if plot==True: PlotXYZ(Simulation,t_max=100, normalize=True,axs=axs)\n",
    "\n",
    "if plot==True:\n",
    "    legend_without_duplicate_labels(axs[1],fontsize=11,ncol=2,loc=8)\n",
    "    fig.tight_layout()\n",
    "    SaveRolloutAsJSON(Simulation.ML_rollout,dataset_name,model_ident,sample_idx)\n",
    "    #print(f\"Maximum Particles outside boundaries: {np.max(ParticlesOutsideBoundary(Simulation.ML_rollout,Simulation.BC_rollout)).astype(int)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotStressComparison\n",
    "fig,axs = PlotStressComparison(Simulation,dims=[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotGraphComparison\n",
    "save = False\n",
    "show = True\n",
    "for t in [-1]:  \n",
    "    fig = PlotGraphComparison(t,Simulation,sample_idx,Simulation.tol,plot_lines=True)\n",
    "    if save == True: plt.savefig(f\"{os.getcwd()}\\\\Figures\\\\Plots\\\\Graph_Sample{sample_idx}_Time{t}_Tol{str(Simulation.tol)[2:]}.png\",bbox_inches='tight')     \n",
    "    if show == True: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import AggregatedRollouts,NormalizedResultantForce\n",
    "from ML_functions import GetModel, MaskTestData\n",
    "\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"Allout\"\n",
    "\n",
    "model = GetModel(dataset_name,model_ident)[0]\n",
    "AggregatedArgs = MaskTestData(dataset_name,\"test\")\n",
    "\n",
    "datalist_ML, datalist_GT,bc_agr = AggregatedRollouts(model,AggregatedArgs,test_dataset_name=dataset_name,device='cpu')\n",
    "MNRF_pred = np.array([[np.mean(NormalizedResultantForce(data)) for data in datalist_sample] for datalist_sample in tqdm(datalist_ML,disable=False)])\n",
    "MNRF_groundtruth = np.array([[np.mean(NormalizedResultantForce(data)) for data in datalist_sample] for datalist_sample in tqdm(datalist_GT,disable=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotForceDistributionComparison\n",
    "quantiles = [10,25,50]\n",
    "fig, ax = PlotForceDistributionComparison(MNRF_groundtruth.T,MNRF_pred.T,quantiles,True)\n",
    "ax[0].set_ylabel('MNRF')\n",
    "ax[0].set_ylim([0,0.6])\n",
    "fig.set_size_inches(10,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenMetricJSON(save_name):\n",
    "    filename = os.path.join(\".\",'Evaluation',f\"{save_name}_metrics.json\")\n",
    "    with open(filename, 'r') as file:\n",
    "        dict = json.load(file)\n",
    "    return dict\n",
    "\n",
    "def SplitMetricDict(dict):\n",
    "    MNRF_dict = {key: dict[key] for key in dict if 'MNRF' in key}\n",
    "    MSE_dict = {key: dict[key] for key in dict if 'MSE' in key}\n",
    "    par_dict = {key: dict[key] for key in dict if 'Par' in key}\n",
    "    return MNRF_dict, MSE_dict, par_dict\n",
    "\n",
    "def PlotExperiment(dict,labels,column,highlight_idx):\n",
    "    MNRF_dict, MSE_dict, par_dict = SplitMetricDict(dict)\n",
    "    MNRF_values = list(MNRF_dict.values())[1:]\n",
    "    MNRF_groundtruth = list(MNRF_dict.values())[0]\n",
    "    MSE_values = list(MSE_dict.values())\n",
    "    par_values = list(par_dict.values())\n",
    "\n",
    "    colors = ['lightgrey']*len(MNRF_values)\n",
    "    colors[highlight_idx] = 'dimgrey'\n",
    "\n",
    "    axs[0,column].bar(labels,MNRF_values,color=colors,width=0.8)\n",
    "    axs[0,column].axhline(MNRF_groundtruth,color='red', linestyle=\"--\",label='Ground truth')\n",
    "    axs[1,column].bar(labels,MSE_values,color=colors,width=0.8)\n",
    "    if axs.shape[0] > 2:\n",
    "        axs[2,column].bar(labels,par_values,color=colors,width=0.8)\n",
    "\n",
    "def SplitDict(dictionary,substrings):\n",
    "    sub_dict = {}\n",
    "    keys = [[key for key in dictionary if substring in key] for substring in substrings]\n",
    "    for key in np.concatenate(keys).tolist():\n",
    "        sub_dict[key] = dictionary[key]\n",
    "    return sub_dict\n",
    "\n",
    "mm=1/25.4\n",
    "fig, axs = plt.subplots(3,3,figsize=(300*mm,150*mm),sharex='col',sharey='row',gridspec_kw={'width_ratios': [4, 16,7]})\n",
    "\n",
    "emb_dict = OpenMetricJSON(save_name='Embedding_Push')\n",
    "PlotExperiment(emb_dict,labels=['16','32','64', '128','256'],column=0,highlight_idx=3)\n",
    "\n",
    "msg_dict = OpenMetricJSON(save_name='N400msg_Push_big')\n",
    "PlotExperiment(msg_dict,labels=[str(i) for i in range(17)],column=1,highlight_idx=5)\n",
    "\n",
    "error_dict = OpenMetricJSON(save_name='Error_Push')\n",
    "substrings = ['bundle','Allout']\n",
    "\n",
    "\n",
    "forward_dict = SplitDict(error_dict,substrings= ['Groundtruth','Emb128','forward5','forward10','forward15','forward20'])\n",
    "bundle_dict = SplitDict(error_dict,substrings= ['bundle','Allout'])\n",
    "\n",
    "plot_dict = forward_dict | bundle_dict\n",
    "bundle = [0,0,0,0,0,3,3]\n",
    "forward = [0,5,10,15,20,0,5] \n",
    "labels = [f\"Bundle: {b:>3}\\nForward: {f:>2}\" for b,f in zip(bundle,forward)]\n",
    "PlotExperiment(plot_dict,labels,column=2,highlight_idx=-1)\n",
    "\n",
    "axs[0,0].set_ylabel('MNRF')\n",
    "axs[1,0].set_ylabel('One-step MSE')\n",
    "axs[-1,0].set_ylabel(\"BC Error\")\n",
    "\n",
    "axs[-1,0].set_xlabel('MLP-size')\n",
    "axs[-1,1].set_xlabel('Message passing steps')\n",
    "\n",
    "axs[0,0].set_ylim([0,1])\n",
    "axs[0,-1].set_ylim([0,1])\n",
    "axs[-1,0].set_yscale(\"log\")\n",
    "\n",
    "axs[-1,-1].tick_params(axis='x',labelrotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.align_ylabels()\n",
    "axs[0,-1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=1/25.4\n",
    "fig, axs = plt.subplots(3,3,figsize=(300*mm,150*mm),sharex='col',sharey='row',gridspec_kw={'width_ratios': [4, 16,7]})\n",
    "\n",
    "emb_dict = OpenMetricJSON(save_name='Embedding_Push')\n",
    "PlotExperiment(emb_dict,labels=['16','32','64', '128','256'],column=0,highlight_idx=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotFnormDistribution\n",
    "Fnorm_GT = MNRF_groundtruth.T\n",
    "Fnorm_ML = MNRF_pred.T\n",
    "fig, ax = plt.subplots(1,1,figsize=(5, 5),sharey=True,sharex=True)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "PlotFnormDistribution(ax,quantiles,Fnorm_GT,\"blue\")\n",
    "ax.legend(ncol=2,title=\"Groundtruth\",title_fontproperties={\"size\":10,\"weight\":\"bold\"})\n",
    "ax.set_ylabel(\"MNRF\")\n",
    "ax.set_xlabel(\"Increment\")\n",
    "\n",
    "PlotFnormDistribution(ax,quantiles,Fnorm_ML,\"red\")\n",
    "leg = ax.legend(ncol=2,title=\"Groundtruth         Model\",title_fontproperties={\"size\":12,\"weight\":\"bold\"})\n",
    "leg._legend_box.align = \"left\"\n",
    "leg.legend_handles[3].set_facecolor([1,0,0,0.2])\n",
    "leg.legend_handles[3].set_edgecolor([1,0,0,0])\n",
    "leg.legend_handles[4].set_facecolor([1,0,0,0.4])\n",
    "leg.legend_handles[4].set_edgecolor([1,0,0,0])\n",
    "\n",
    "g=0.7\n",
    "r=0.4\n",
    "leg.legend_handles[0].set_facecolor([r,g,1,0.3])\n",
    "leg.legend_handles[0].set_edgecolor([r,g,1,0])\n",
    "leg.legend_handles[1].set_facecolor([r,g,1,0.6])\n",
    "leg.legend_handles[1].set_edgecolor([r,g,1,0])\n",
    "\n",
    "ax.set_xlabel(\"Increment\")\n",
    "ax.set_ylim([0,1])\n",
    "fig.savefig(r\"C:\\Users\\20182319\\OneDrive - TU Eindhoven\\Graduation\\Figures\\Experiment_Error.png\",dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotFres\n",
    "from Evaluation import NormalizedResultantForce\n",
    "list = [NormalizedResultantForce(data) for data in Simulation.GroundTruth]\n",
    "Fnorm_ML = np.array([NormalizedResultantForce(data) for data in Simulation.ML_rollout])\n",
    "Fnorm_GT = np.array([NormalizedResultantForce(data) for data in Simulation.GroundTruth])\n",
    "fig = PlotFres(np.mean(Fnorm_GT,axis=1),np.mean(Fnorm_ML,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotForceDistributionComparison\n",
    "quantiles = [5,10,25,50]\n",
    "fig, ax = PlotForceDistributionComparison(Fnorm_GT,Fnorm_ML,quantiles,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetInternalStressRollout\n",
    "stress_evo = GetInternalStressRollout(Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "from Plotting import MakeGIF, PlotMeshNormals\n",
    "\n",
    "gifname = f\"{dataset_name}_GT_deform\"\n",
    "gifname = f\"{dataset_name}_{model_ident}_deform\"\n",
    "\n",
    "datalist = Simulation.GroundTruth\n",
    "datalist = Simulation.ML_rollout\n",
    "MakeGIF(datalist,gifname,fps=8,color='lightblue',deformation=True)\n",
    "\n",
    "#data = Rollout.ML_rollout[10]\n",
    "#data = Rollout.GroundTruth[0]\n",
    "#PlotMeshNormals(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking topology functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ConstructTopology\n",
    "from ML_functions import MaskTestData\n",
    "dataset_name = 'N400_Mono'\n",
    "AggregatedArgs = MaskTestData(dataset_name,'test')\n",
    "data,top,bc = AggregatedArgs\n",
    "data = data[0]\n",
    "top = top[0]\n",
    "bc = bc[0]\n",
    "t=0\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "\n",
    "\n",
    "par_data = data[0]\n",
    "super_topology = ConstructTopology(par_data,BC_t,6)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,top,bc = AggregatedArgs\n",
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import TopologyFromPlausibleTopology_old, TopologyFromPlausibleTopology\n",
    "\n",
    "tol=0\n",
    "t0 = time.time()\n",
    "topology_0 = ConstructTopology(par_data,BC_t,tol)-1\n",
    "t1 = time.time()\n",
    "topology_1 = TopologyFromPlausibleTopology_old(super_topology,par_data,BC_t,tol)\n",
    "t2 = time.time()\n",
    "topology_2 = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,tol)\n",
    "t3 = time.time()\n",
    "\n",
    "print(np.all(topology_0 == topology_1))\n",
    "print(np.all(topology_1 == topology_2))\n",
    "print(t1-t0)\n",
    "print(t2-t1)\n",
    "print(t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregatedArgs = MaskTestData(dataset_name)\n",
    "data,top,bc = AggregatedArgs\n",
    "par_data = data[0][0]\n",
    "print(par_data)\n",
    "R_avg = par_data[0,3]\n",
    "noise_factor = 0.01\n",
    "standard_deviation = noise_factor*R_avg\n",
    "noise = np.array(standard_deviation*torch.randn((par_data.shape[0],3)))\n",
    "par_data[:,:3]+=noise\n",
    "print(par_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=40\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "bc=BC_t\n",
    "par_data = data[t]\n",
    "top0 = ConstructTopology(par_data,bc,0)-1\n",
    "\n",
    "from Encoding import TopologyFromPlausibleTopology\n",
    "topology_sub = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,0)\n",
    "\n",
    "topology_sub == top0,topology_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetGamma\n",
    "gamma = GetGamma(data)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageDX(data):\n",
    "    norm = torch.norm(data.y,dim=1)\n",
    "    return torch.mean(norm)\n",
    "\n",
    "dataset_test.y.abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDatasetStats(dataset):\n",
    "    torch.set_printoptions(sci_mode=False, linewidth=150)\n",
    "    print(f\"x_max:      {dataset.x.max(dim=0)[0]}\")\n",
    "    print(f\"y_mean:     {dataset.y.mean(dim=0)}\")\n",
    "    print(f\"y_std:      {dataset.y.std(dim=0)}\")\n",
    "    print(f\"edge_mean:  {dataset.edge_attr.mean(dim=0)}\")\n",
    "    print(f\"edge_std:   {dataset.edge_attr.std(dim=0)}\")\n",
    "    print(\"\\n\")\n",
    "    torch.set_printoptions(profile='default')\n",
    "\n",
    "PrintDatasetStats(dataset_train)\n",
    "#PrintDatasetStats(dataset_train_push)\n",
    "\n",
    "fig = plt.hist(dataset_train_push.push_forward_steps,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterStart(dataset):\n",
    "    idx = np.nonzero([data.time == 0 for data in dataset])\n",
    "    return torch.utils.data.Subset(dataset_test,idx)[0]\n",
    "\n",
    "#dataset_test_start = FilterStart(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ToPytorchData, GetLength\n",
    "\n",
    "def GetLimits(data):\n",
    "    max = [torch.max(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    min = [torch.min(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    max = torch.stack(max)\n",
    "    min = torch.stack(min)\n",
    "    limits = torch.stack([min,max],dim=1)\n",
    "    return limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import load\n",
    "def SplitData(dataset_name,data_split):\n",
    "    loaded_data = load(dataset_name)\n",
    "    splits=np.array(data_split)*loaded_data[0].shape[0]\n",
    "    test_data = [np.split(data,splits.astype(int))[2] for data in loaded_data]\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetContactForce\n",
    "from Encoding import ConvertToDirected\n",
    "data = Rollout.GroundTruth[0].clone()\n",
    "force = GetContactForce(data)\n",
    "GT = np.loadtxt('PairContact001.txt')\n",
    "\n",
    "data =ConvertToDirected(data)\n",
    "force2 = GetContactForce(data)\n",
    "\n",
    "print(torch.norm(force,dim=1).size())\n",
    "print(torch.norm(force2,dim=1))\n",
    "\n",
    "torch.all(np.round(GT[:,2],2)==torch.round(torch.norm(force2,dim=1),decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import GetInternalStressRollout\n",
    "stress = GetInternalStressRollout(Rollout)\n",
    "torch.set_printoptions(4)\n",
    "print(\"Stress at time 0\")\n",
    "print(torch.round(stress[0],decimals=8)),\n",
    "print(\"\\nStress at time 99\")\n",
    "print(torch.round(stress[-1],decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotContactVectorAndForce, GetAllContactpoints,AxesLimits\n",
    "data = Rollout.GroundTruth[0]\n",
    "BC = Rollout.BC_rollout[3]\n",
    "fig,axs = PlotContactVectorAndForce(data,BC)\n",
    "for ax in axs: AxesLimits(ax,BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformMLP(torch.nn.Sequential):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_hidden_layers,activation_function=torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_hidden_layers = num_hidden_layers,\n",
    "        self.activation_function=activation_function\n",
    "        hidden_input_size = input_size\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.add_module(f\"dense{i}\",torch.nn.Linear(hidden_input_size,hidden_size))\n",
    "            self.add_module(f\"act{i}\",activation_function)\n",
    "            hidden_input_size = hidden_size\n",
    "        self.add_module(f\"output\",torch.nn.Linear(hidden_size,output_size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEM-GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
