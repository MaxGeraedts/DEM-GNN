{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM-GNN Rollout Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import json \n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from torch_geometric.nn.models import MLP\n",
    "print(torch.cuda.is_available())\n",
    "np.set_printoptions(linewidth=200)\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_functions import DEM_Dataset\n",
    "\n",
    "force_reload    = True\n",
    "dataset_name    = \"Pill_Mono\"\n",
    "bundle_size     = 1\n",
    "\n",
    "pre_transform = T.Compose([T.Cartesian(False),\n",
    "                           T.Distance(norm=False,cat=True)])\n",
    "\n",
    "[dataset_train]      = [DEM_Dataset(dataset_name,\n",
    "                                                               dataset_type,\n",
    "                                                               force_reload     = force_reload,\n",
    "                                                               pre_transform    = pre_transform,\n",
    "                                                               bundle_size      = bundle_size) \n",
    "                                                               for dataset_type in [\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotTrainingLoss\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"Emb128\"\n",
    "fig, axs = PlotTrainingLoss(dataset_name,model_ident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import CompareModels, Evaluation\n",
    "Evaluation_function=Evaluation(mode='mechanics_mean',print_results=True)\n",
    "#model_idents = [\"Allout\",'Forward_Bundle','Forward','Bundle', 'Onestep']\n",
    "metrics = CompareModels(dataset_name        = 'N400_Mono',\n",
    "                        model_idents        = ['Emb16','Emb32','Emb64','Emb128'],\n",
    "                        Evaluation_function = Evaluation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = CompareModels(dataset_name        = '2Sphere',\n",
    "                        model_idents        = ['Allout'],\n",
    "                        Evaluation_function = Evaluation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model N400_Mono_Emb16\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\20182319\\\\Documents\\\\Master\\\\Graduation\\\\Repos\\\\DEM-GNN\\\\Data\\\\raw\\\\N400_Mono_Data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m model_ident \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmb16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m GetModel(dataset_name,model_ident)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m AggregatedArgs \u001b[38;5;241m=\u001b[39m MaskTestData(dataset_name,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\20182319\\Documents\\Master\\Graduation\\Repos\\DEM-GNN\\ML_functions.py:28\u001b[0m, in \u001b[0;36mMaskTestData\u001b[1;34m(dataset_name, dataset_type)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Masks out data from rawdata arrays\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    tuple: [data, topology, boundary_conditions]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m type_dic\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m---> 28\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m [load(dataset_name,dataset_type) \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpar_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbc\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     29\u001b[0m mask \u001b[38;5;241m=\u001b[39m DataMask(loaded_data[\u001b[38;5;241m0\u001b[39m],)[type_dic[dataset_type]]\n\u001b[0;32m     30\u001b[0m test_data \u001b[38;5;241m=\u001b[39m [data[mask] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loaded_data]\n",
      "File \u001b[1;32mc:\\Users\\20182319\\Documents\\Master\\Graduation\\Repos\\DEM-GNN\\ML_functions.py:28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Masks out data from rawdata arrays\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    tuple: [data, topology, boundary_conditions]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m type_dic\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m---> 28\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m [load(dataset_name,dataset_type) \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpar_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbc\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     29\u001b[0m mask \u001b[38;5;241m=\u001b[39m DataMask(loaded_data[\u001b[38;5;241m0\u001b[39m],)[type_dic[dataset_type]]\n\u001b[0;32m     30\u001b[0m test_data \u001b[38;5;241m=\u001b[39m [data[mask] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loaded_data]\n",
      "File \u001b[1;32mc:\\Users\\20182319\\Documents\\Master\\Graduation\\Repos\\DEM-GNN\\Encoding.py:178\u001b[0m, in \u001b[0;36mload\u001b[1;34m(dataset_name, data_type)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mLoads aggregated data from disk\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    Tuple: [data, top , bc]\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpar_data\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Data.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    180\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Topology.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\20182319\\AppData\\Local\\anaconda3\\envs\\DEM-GNN2\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mfspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\20182319\\\\Documents\\\\Master\\\\Graduation\\\\Repos\\\\DEM-GNN\\\\Data\\\\raw\\\\N400_Mono_Data.npy'"
     ]
    }
   ],
   "source": [
    "from Plotting import PlotXYZ, PlotStressComparison\n",
    "from ML_functions import LearnedSimulator, NormalizeData, GetModel, Rescale, NormalizePos, MaskTestData\n",
    "from Encoding import SaveRolloutAsJSON\n",
    "\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"Emb16\"\n",
    "\n",
    "model = GetModel(dataset_name,model_ident)[0]\n",
    "AggregatedArgs = MaskTestData(dataset_name,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_name = f\"{dataset_name}_bund{model.bundle_size}\"\n",
    "transform = T.Compose([T.Cartesian(False),T.Distance(norm=False,cat=True),NormalizeData(dataset_name,scale_name)])\n",
    "Simulation = LearnedSimulator(model, scale_function = Rescale(dataset_name,scale_name),transform = transform)\n",
    "for sample_idx in [0]:\n",
    "    Simulation.Rollout(*AggregatedArgs,sample_idx,show_tqdm=True)\n",
    "    if dataset_name == \"2Sphere\": \n",
    "        PlotXYZ(Simulation,t_max=100, normalize=False)\n",
    "        #PlotStressComparison(Simulation,plot_ml=True)\n",
    "    SaveRolloutAsJSON(Simulation.ML_rollout,dataset_name,model_ident,sample_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotGraphComparison\n",
    "save = False\n",
    "show = True\n",
    "for t in range(0,100,20):  \n",
    "    fig = PlotGraphComparison(t,Simulation,sample_idx,Simulation.tol,plot_lines=True)\n",
    "    if save == True: plt.savefig(f\"{os.getcwd()}\\\\Figures\\\\Plots\\\\Graph_Sample{sample_idx}_Time{t}_Tol{str(Simulation.tol)[2:]}.png\",bbox_inches='tight')     \n",
    "    if show == True: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotFres\n",
    "from Evaluation import NormalizedResultantForce\n",
    "list = [NormalizedResultantForce(data) for data in Simulation.GroundTruth]\n",
    "Fnorm_ML = np.array([NormalizedResultantForce(data) for data in Simulation.ML_rollout])\n",
    "Fnorm_GT = np.array([NormalizedResultantForce(data) for data in Simulation.GroundTruth])\n",
    "fig = PlotFres(np.mean(Fnorm_GT,axis=1),np.mean(Fnorm_ML,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotStressComparison\n",
    "PlotStressComparison(Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotForceDistributionComparison\n",
    "quantiles = [5,10,25,50]\n",
    "fig, ax = PlotForceDistributionComparison(Fnorm_GT,Fnorm_ML,quantiles,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "from Plotting import MakeGIF, PlotMeshNormals\n",
    "\n",
    "gifname = f\"{dataset_name}_GT_deform\"\n",
    "gifname = f\"{dataset_name}_{model_ident}_deform\"\n",
    "\n",
    "datalist = Simulation.GroundTruth\n",
    "datalist = Simulation.ML_rollout\n",
    "MakeGIF(datalist,gifname,fps=8,color='lightblue',deformation=True)\n",
    "\n",
    "#data = Rollout.ML_rollout[10]\n",
    "#data = Rollout.GroundTruth[0]\n",
    "#PlotMeshNormals(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking topology functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ConstructTopology\n",
    "from ML_functions import MaskTestData\n",
    "dataset_name = 'N400_Mono'\n",
    "AggregatedArgs = MaskTestData(dataset_name,'test')\n",
    "data,top,bc = AggregatedArgs\n",
    "data = data[0]\n",
    "top = top[0]\n",
    "bc = bc[0]\n",
    "t=0\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "\n",
    "\n",
    "par_data = data[0]\n",
    "super_topology = ConstructTopology(par_data,BC_t,6)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,top,bc = AggregatedArgs\n",
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import TopologyFromPlausibleTopology_old, TopologyFromPlausibleTopology\n",
    "\n",
    "tol=0\n",
    "t0 = time.time()\n",
    "topology_0 = ConstructTopology(par_data,BC_t,tol)-1\n",
    "t1 = time.time()\n",
    "topology_1 = TopologyFromPlausibleTopology_old(super_topology,par_data,BC_t,tol)\n",
    "t2 = time.time()\n",
    "topology_2 = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,tol)\n",
    "t3 = time.time()\n",
    "\n",
    "print(np.all(topology_0 == topology_1))\n",
    "print(np.all(topology_1 == topology_2))\n",
    "print(t1-t0)\n",
    "print(t2-t1)\n",
    "print(t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregatedArgs = MaskTestData(dataset_name)\n",
    "data,top,bc = AggregatedArgs\n",
    "par_data = data[0][0]\n",
    "print(par_data)\n",
    "R_avg = par_data[0,3]\n",
    "noise_factor = 0.01\n",
    "standard_deviation = noise_factor*R_avg\n",
    "noise = np.array(standard_deviation*torch.randn((par_data.shape[0],3)))\n",
    "par_data[:,:3]+=noise\n",
    "print(par_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=40\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "bc=BC_t\n",
    "par_data = data[t]\n",
    "top0 = ConstructTopology(par_data,bc,0)-1\n",
    "\n",
    "from Encoding import TopologyFromPlausibleTopology\n",
    "topology_sub = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,0)\n",
    "\n",
    "topology_sub == top0,topology_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetGamma\n",
    "gamma = GetGamma(data)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageDX(data):\n",
    "    norm = torch.norm(data.y,dim=1)\n",
    "    return torch.mean(norm)\n",
    "\n",
    "dataset_test.y.abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDatasetStats(dataset):\n",
    "    torch.set_printoptions(sci_mode=False, linewidth=150)\n",
    "    print(f\"x_max:      {dataset.x.max(dim=0)[0]}\")\n",
    "    print(f\"y_mean:     {dataset.y.mean(dim=0)}\")\n",
    "    print(f\"y_std:      {dataset.y.std(dim=0)}\")\n",
    "    print(f\"edge_mean:  {dataset.edge_attr.mean(dim=0)}\")\n",
    "    print(f\"edge_std:   {dataset.edge_attr.std(dim=0)}\")\n",
    "    print(\"\\n\")\n",
    "    torch.set_printoptions(profile='default')\n",
    "\n",
    "PrintDatasetStats(dataset_train)\n",
    "#PrintDatasetStats(dataset_train_push)\n",
    "\n",
    "fig = plt.hist(dataset_train_push.push_forward_steps,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterStart(dataset):\n",
    "    idx = np.nonzero([data.time == 0 for data in dataset])\n",
    "    return torch.utils.data.Subset(dataset_test,idx)[0]\n",
    "\n",
    "#dataset_test_start = FilterStart(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ToPytorchData, GetLength\n",
    "\n",
    "def GetLimits(data):\n",
    "    max = [torch.max(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    min = [torch.min(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    max = torch.stack(max)\n",
    "    min = torch.stack(min)\n",
    "    limits = torch.stack([min,max],dim=1)\n",
    "    return limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import load\n",
    "def SplitData(dataset_name,data_split):\n",
    "    loaded_data = load(dataset_name)\n",
    "    splits=np.array(data_split)*loaded_data[0].shape[0]\n",
    "    test_data = [np.split(data,splits.astype(int))[2] for data in loaded_data]\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetContactForce\n",
    "from Encoding import ConvertToDirected\n",
    "data = Rollout.GroundTruth[0].clone()\n",
    "force = GetContactForce(data)\n",
    "GT = np.loadtxt('PairContact001.txt')\n",
    "\n",
    "data =ConvertToDirected(data)\n",
    "force2 = GetContactForce(data)\n",
    "\n",
    "print(torch.norm(force,dim=1).size())\n",
    "print(torch.norm(force2,dim=1))\n",
    "\n",
    "torch.all(np.round(GT[:,2],2)==torch.round(torch.norm(force2,dim=1),decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import GetInternalStressRollout\n",
    "stress = GetInternalStressRollout(Rollout)\n",
    "torch.set_printoptions(4)\n",
    "print(\"Stress at time 0\")\n",
    "print(torch.round(stress[0],decimals=8)),\n",
    "print(\"\\nStress at time 99\")\n",
    "print(torch.round(stress[-1],decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotContactVectorAndForce, GetAllContactpoints,AxesLimits\n",
    "data = Rollout.GroundTruth[0]\n",
    "BC = Rollout.BC_rollout[3]\n",
    "fig,axs = PlotContactVectorAndForce(data,BC)\n",
    "for ax in axs: AxesLimits(ax,BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformMLP(torch.nn.Sequential):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_hidden_layers,activation_function=torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_hidden_layers = num_hidden_layers,\n",
    "        self.activation_function=activation_function\n",
    "        hidden_input_size = input_size\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.add_module(f\"dense{i}\",torch.nn.Linear(hidden_input_size,hidden_size))\n",
    "            self.add_module(f\"act{i}\",activation_function)\n",
    "            hidden_input_size = hidden_size\n",
    "        self.add_module(f\"output\",torch.nn.Linear(hidden_size,output_size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEM-GNN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
