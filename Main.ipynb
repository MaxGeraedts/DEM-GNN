{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM-GNN Rollout Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "torch.cuda.is_available()\n",
    "np.set_printoptions(linewidth=200)\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_functions import MaskTestData\n",
    "[data, top, bc] = MaskTestData(\"2Sphere\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, 6)\n",
      "[[-0.00000001 -0.00000023 -0.00098313]\n",
      " [-0.00000006  0.0000001   0.00098313]]\n",
      "\n",
      "[[[-0.00000001 -0.00000023 -0.00098313]\n",
      "  [-0.00000006  0.0000001   0.00098313]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_0 = data[0]\n",
    "bundle_size = 1\n",
    "push_forward_steps = 0\n",
    "t=0\n",
    "print(data_0.shape)\n",
    "\n",
    "def SliceAndReshapeData(par_data,t, bundle_size,push_forward_steps):\n",
    "    pos_slice = par_data[t+push_forward_steps*bundle_size:t+(push_forward_steps+1)*bundle_size,:,:3]\n",
    "    pos_slice_2D = np.reshape(np.swapaxes(pos_slice,0,1),(-1,3*bundle_size))\n",
    "    return pos_slice_2D\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "par_data = data_0\n",
    "\n",
    "pos_slice = par_data[t+push_forward_steps*bundle_size:t+(push_forward_steps+1)*bundle_size,:,:3]\n",
    "pos_target_slice = par_data[t+1+push_forward_steps*bundle_size:t+1+(push_forward_steps+1)*bundle_size,:,:3]\n",
    "displacements = pos_target_slice-pos_slice\n",
    "displacements = np.reshape(np.swapaxes(displacements,0,1),(-1,3*bundle_size)).astype(float)\n",
    "\n",
    "reshaped = np.swapaxes(np.reshape(displacements,(-1,bundle_size,3)),0,1)\n",
    "print(f\"{displacements}\\n\")\n",
    "print(f\"{reshaped}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9800 [03:32<?, ?it/s]\n",
      "100%|██████████| 1700/1700 [01:48<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166600/166600 [00:34<00:00, 4875.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166600/166600 [00:32<00:00, 5127.87it/s]\n",
      "Done!\n",
      "c:\\Users\\20182319\\AppData\\Local\\anaconda3\\envs\\DEM-GNN2\\Lib\\site-packages\\torch_geometric\\io\\fs.py:229: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([_reconstruct])` to allowlist this global.\n",
      "  warnings.warn(f\"{warn_msg} Please use \"\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting validate data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:11<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing validate data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19600/19600 [00:03<00:00, 5071.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing validate data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19600/19600 [00:03<00:00, 5447.53it/s]\n",
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9800/9800 [00:01<00:00, 4976.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9800/9800 [00:01<00:00, 5399.51it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m train           \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m [dataset_train, dataset_val, dataset_test] \u001b[38;5;241m=\u001b[39m [DEM_Dataset(dataset_name,\n\u001b[0;32m     12\u001b[0m                                                         dataset_type,\n\u001b[0;32m     13\u001b[0m                                                         force_reload     \u001b[38;5;241m=\u001b[39m force_reload,\n\u001b[0;32m     14\u001b[0m                                                         pre_transform    \u001b[38;5;241m=\u001b[39m pre_transform,\n\u001b[0;32m     15\u001b[0m                                                         bundle_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m     16\u001b[0m                                                         \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m---> 17\u001b[0m [dataset_train_push]  \u001b[38;5;241m=\u001b[39m [DEM_Dataset(dataset_name,\n\u001b[0;32m     18\u001b[0m                                     dataset_type,\n\u001b[0;32m     19\u001b[0m                                     force_reload     \u001b[38;5;241m=\u001b[39m force_reload,\n\u001b[0;32m     20\u001b[0m                                     pre_transform    \u001b[38;5;241m=\u001b[39m pre_transform,\n\u001b[0;32m     21\u001b[0m                                     push_forward_step_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     22\u001b[0m                                     model \u001b[38;5;241m=\u001b[39m GetModel(model_name)) \n\u001b[0;32m     23\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m GetModel(dataset_name,model_ident,edge_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m train           \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m [dataset_train, dataset_val, dataset_test] \u001b[38;5;241m=\u001b[39m [DEM_Dataset(dataset_name,\n\u001b[0;32m     12\u001b[0m                                                         dataset_type,\n\u001b[0;32m     13\u001b[0m                                                         force_reload     \u001b[38;5;241m=\u001b[39m force_reload,\n\u001b[0;32m     14\u001b[0m                                                         pre_transform    \u001b[38;5;241m=\u001b[39m pre_transform,\n\u001b[0;32m     15\u001b[0m                                                         bundle_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m     16\u001b[0m                                                         \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m---> 17\u001b[0m [dataset_train_push]  \u001b[38;5;241m=\u001b[39m [DEM_Dataset(dataset_name,\n\u001b[0;32m     18\u001b[0m                                     dataset_type,\n\u001b[0;32m     19\u001b[0m                                     force_reload     \u001b[38;5;241m=\u001b[39m force_reload,\n\u001b[0;32m     20\u001b[0m                                     pre_transform    \u001b[38;5;241m=\u001b[39m pre_transform,\n\u001b[0;32m     21\u001b[0m                                     push_forward_step_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     22\u001b[0m                                     model \u001b[38;5;241m=\u001b[39m GetModel(model_name)) \n\u001b[0;32m     23\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m GetModel(dataset_name,model_ident,edge_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\20182319\\Documents\\Master\\Graduation\\Repos\\DEM-GNN\\ML_functions.py:228\u001b[0m, in \u001b[0;36mDEM_Dataset.__init__\u001b[1;34m(self, file_name, Dataset_type, mode, force_reload, pre_transform, transform, pre_filter, root, super_tol, tol, noise_factor, push_forward_step_max, bundle_size, model)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbundle_size \u001b[38;5;241m=\u001b[39m bundle_size\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform,pre_filter,force_reload\u001b[38;5;241m=\u001b[39mforce_reload)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data_path,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_file_names[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\20182319\\AppData\\Local\\anaconda3\\envs\\DEM-GNN2\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter, log,\n\u001b[0;32m     82\u001b[0m                      force_reload)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20182319\\AppData\\Local\\anaconda3\\envs\\DEM-GNN2\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:115\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[1;32mc:\\Users\\20182319\\AppData\\Local\\anaconda3\\envs\\DEM-GNN2\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:262\u001b[0m, in \u001b[0;36mDataset._process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m    261\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[0;32m    264\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    265\u001b[0m fs\u001b[38;5;241m.\u001b[39mtorch_save(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "File \u001b[1;32mc:\\Users\\20182319\\Documents\\Master\\Graduation\\Repos\\DEM-GNN\\ML_functions.py:263\u001b[0m, in \u001b[0;36mDEM_Dataset.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     simulations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_filter(simulations)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_step_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 263\u001b[0m     Simulation \u001b[38;5;241m=\u001b[39m LearnedSimulator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,Rescale(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuper_tol,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    264\u001b[0m                                   transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform,NormalizeData(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name)]))\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRollout_step \u001b[38;5;241m=\u001b[39m Simulation\u001b[38;5;241m.\u001b[39mRollout_Step\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\20182319\\Documents\\Master\\Graduation\\Repos\\DEM-GNN\\ML_functions.py:40\u001b[0m, in \u001b[0;36mLearnedSimulator.__init__\u001b[1;34m(self, model, scale_function, super_tol, tol, transform, timesteps)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,model,scale_function,super_tol:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,tol:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,timesteps:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale \u001b[38;5;241m=\u001b[39m scale_function\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m=\u001b[39m timesteps\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from ML_functions import DEM_Dataset, Trainer, GetModel\n",
    "dataset_name=\"2Sphere\"\n",
    "model_ident = \"NewModel_2\"\n",
    "model_name=f\"{dataset_name}_{model_ident}\"\n",
    "pre_transform = T.Compose([T.Cartesian(False),\n",
    "                           T.Distance(norm=False,cat=True)])\n",
    "transform       = None\n",
    "force_reload    = True\n",
    "train           = False\n",
    "\n",
    "[dataset_train, dataset_val, dataset_test] = [DEM_Dataset(dataset_name,\n",
    "                                                        dataset_type,\n",
    "                                                        force_reload     = force_reload,\n",
    "                                                        pre_transform    = pre_transform,\n",
    "                                                        bundle_size=3) \n",
    "                                                        for dataset_type in [\"train\",\"validate\",\"test\"]]\n",
    "[dataset_train_push]  = [DEM_Dataset(dataset_name,\n",
    "                                    dataset_type,\n",
    "                                    force_reload     = force_reload,\n",
    "                                    pre_transform    = pre_transform,\n",
    "                                    push_forward_step_max=8,\n",
    "                                    model = GetModel(model_name)[0]) \n",
    "                                    for dataset_type in [\"train\"]]\n",
    "\n",
    "if train == True:\n",
    "    model = GetModel(dataset_name,model_ident,edge_dim=4)\n",
    "    trainer = Trainer(model, dataset_train,dataset_val,\n",
    "                      batch_size=32,\n",
    "                      lr=0.0000001,\n",
    "                      epochs=2000,\n",
    "                      model_name=f\"{dataset_name}{model_ident}\")\n",
    "    #trainer.train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9184e-05, -7.2542e-04, -1.0635e+00, -1.3949e-03,  1.1995e-03, -1.0627e+00],\n",
       "        [-1.8379e-04,  3.0852e-04,  1.0635e+00,  1.6175e-03, -1.1376e-03,  1.0627e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotTrainingLoss\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"NewModel_2_Push\"\n",
    "try: fig, axs = PlotTrainingLoss(dataset_name,model_ident)\n",
    "except: print(\"Failed plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_functions import NormalizeData, MaskTestData\n",
    "from Evaluation import Evaluate\n",
    "dataset_name=\"N400_Mono\"\n",
    "transform = T.Compose([T.Cartesian(False),T.Distance(norm=False,cat=True),NormalizeData(dataset_name)])\n",
    "AggregatedArgs = MaskTestData(dataset_name,\"test\")\n",
    "metrics = Evaluate(dataset_name,\"NewModel_1\",transform, AggregatedArgs)\n",
    "metrics = Evaluate(dataset_name,\"NewModel_2\",transform, AggregatedArgs)\n",
    "metrics = Evaluate(dataset_name,\"NewModel_2_Push\",transform, AggregatedArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Evaluate(\"2Sphere\",\"NewModel_1\",transform)\n",
    "metrics = Evaluate(\"2Sphere\",\"NewModel_2\",transform)\n",
    "metrics = Evaluate(\"2Sphere\",\"NewModel_2_Push\",transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotXYZ\n",
    "from ML_functions import LearnedSimulator, NormalizeData, GetModel, Rescale, NormalizePos, MaskTestData\n",
    "\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"NewModel_2_Push\"\n",
    "\n",
    "model_name = f\"{dataset_name}_{model_ident}\"\n",
    "model = GetModel(model_name)[0]\n",
    "AggregatedArgs = MaskTestData(dataset_name,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Cartesian(False),T.Distance(norm=False,cat=True),NormalizeData(dataset_name)])\n",
    "Simulation = LearnedSimulator(model, scale_function = Rescale(dataset_name),transform = transform)\n",
    "for sample_idx in [0]:\n",
    "    Simulation.Rollout(*AggregatedArgs,sample_idx,show_tqdm=True)\n",
    "    if dataset_name == \"2Sphere\": \n",
    "        PlotXYZ(Simulation,t_max=100, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotGraphComparison\n",
    "save = False\n",
    "show = True\n",
    "for t in range(0,100,5):  \n",
    "    fig = PlotGraphComparison(t,Simulation,sample_idx,Simulation.tol,plot_lines=True)\n",
    "    if save == True: plt.savefig(f\"{os.getcwd()}\\\\Figures\\\\Plots\\\\Graph_Sample{sample_idx}_Time{t}_Tol{str(Simulation.tol)[2:]}.png\",bbox_inches='tight')     \n",
    "    if show == True: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotFres\n",
    "from Evaluation import AggregateForces\n",
    "Fcontact_GT,Fres_GT, Fnorm_GT, Fsum_GT = AggregateForces(Simulation.GroundTruth)\n",
    "Fcontact_ML,Fres_ML, Fnorm_ML, Fsum_ML = AggregateForces(Simulation.ML_rollout)\n",
    "fig = PlotFres(Fsum_GT,Fsum_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotStressComparison\n",
    "fig, axs = PlotStressComparison(Simulation,Fcontact_GT,Fcontact_ML,Plot_ML=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotForceDistributionComparison\n",
    "quantiles = [1,5,10,25,50]\n",
    "fig, ax = PlotForceDistributionComparison(Fnorm_GT,Fnorm_ML,quantiles,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "from Plotting import MakeGIF, PlotMeshNormals\n",
    "\n",
    "\n",
    "#datalist = Rollout.ML_rollout\n",
    "#gifname = f\"ML_2_{sample_idx}_Tol{str(tol)[2:]}\"\n",
    "gifname = f\"{dataset_name}_{model_ident}_deform\"\n",
    "datalist = Simulation.GroundTruth\n",
    "#datalist = Rollout.ML_rollout\n",
    "MakeGIF(datalist,gifname,fps=8,color='lightblue',deformation=True)\n",
    "\n",
    "#data = Rollout.ML_rollout[10]\n",
    "#data = Rollout.GroundTruth[0]\n",
    "#PlotMeshNormals(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking topology functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ConstructTopology\n",
    "from ML_functions import MaskTestData\n",
    "dataset_name = 'N400'\n",
    "AggregatedArgs = MaskTestData(dataset_name,'test')\n",
    "data,top,bc = AggregatedArgs\n",
    "data = data[0]\n",
    "top = top[0]\n",
    "bc = bc[0]\n",
    "t=0\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "\n",
    "\n",
    "par_data = data[0]\n",
    "super_topology = ConstructTopology(par_data,BC_t,6)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import TopologyFromPlausibleTopology_old, TopologyFromPlausibleTopology\n",
    "\n",
    "tol=0\n",
    "t0 = time.time()\n",
    "topology_0 = ConstructTopology(par_data,BC_t,tol)-1\n",
    "t1 = time.time()\n",
    "topology_1 = TopologyFromPlausibleTopology_old(super_topology,par_data,BC_t,tol)\n",
    "t2 = time.time()\n",
    "topology_2 = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,tol)\n",
    "t3 = time.time()\n",
    "\n",
    "print(np.all(topology_0 == topology_1))\n",
    "print(np.all(topology_1 == topology_2))\n",
    "print(t1-t0)\n",
    "print(t2-t1)\n",
    "print(t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregatedArgs = MaskTestData(dataset_name)\n",
    "data,top,bc = AggregatedArgs\n",
    "par_data = data[0][0]\n",
    "print(par_data)\n",
    "R_avg = par_data[0,3]\n",
    "noise_factor = 0.01\n",
    "standard_deviation = noise_factor*R_avg\n",
    "noise = np.array(standard_deviation*torch.randn((par_data.shape[0],3)))\n",
    "par_data[:,:3]+=noise\n",
    "print(par_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=40\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "bc=BC_t\n",
    "par_data = data[t]\n",
    "top0 = ConstructTopology(par_data,bc,0)-1\n",
    "\n",
    "from Encoding import TopologyFromPlausibleTopology\n",
    "topology_sub = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,0)\n",
    "\n",
    "topology_sub == top0,topology_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetGamma\n",
    "gamma = GetGamma(data)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageDX(data):\n",
    "    norm = torch.norm(data.y,dim=1)\n",
    "    return torch.mean(norm)\n",
    "\n",
    "dataset_test.y.abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_max:      tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], dtype=torch.float64)\n",
      "y_mean:     tensor([    -0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000,      0.0000], dtype=torch.float64)\n",
      "y_std:      tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], dtype=torch.float64)\n",
      "edge_mean:  tensor([     0.0000,      0.0000,     -0.0000,      0.0000], dtype=torch.float64)\n",
      "edge_std:   tensor([1.0000, 1.0000, 1.0000, 1.0000], dtype=torch.float64)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train_push' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m PrintDatasetStats(dataset_train)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#PrintDatasetStats(dataset_train_push)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mhist(dataset_train_push\u001b[38;5;241m.\u001b[39mpush_forward_steps,\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_train_push' is not defined"
     ]
    }
   ],
   "source": [
    "def PrintDatasetStats(dataset):\n",
    "    torch.set_printoptions(sci_mode=False, linewidth=150)\n",
    "    print(f\"x_max:      {dataset.x.max(dim=0)[0]}\")\n",
    "    print(f\"y_mean:     {dataset.y.mean(dim=0)}\")\n",
    "    print(f\"y_std:      {dataset.y.std(dim=0)}\")\n",
    "    print(f\"edge_mean:  {dataset.edge_attr.mean(dim=0)}\")\n",
    "    print(f\"edge_std:   {dataset.edge_attr.std(dim=0)}\")\n",
    "    print(\"\\n\")\n",
    "    torch.set_printoptions(profile='default')\n",
    "\n",
    "PrintDatasetStats(dataset_train)\n",
    "#PrintDatasetStats(dataset_train_push)\n",
    "\n",
    "fig = plt.hist(dataset_train_push.push_forward_steps,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterStart(dataset):\n",
    "    idx = np.nonzero([data.time == 0 for data in dataset])\n",
    "    return torch.utils.data.Subset(dataset_test,idx)[0]\n",
    "\n",
    "#dataset_test_start = FilterStart(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ToPytorchData, GetLength\n",
    "\n",
    "def GetLimits(data):\n",
    "    max = [torch.max(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    min = [torch.min(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    max = torch.stack(max)\n",
    "    min = torch.stack(min)\n",
    "    limits = torch.stack([min,max],dim=1)\n",
    "    return limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import load\n",
    "def SplitData(dataset_name,data_split):\n",
    "    loaded_data = load(dataset_name)\n",
    "    splits=np.array(data_split)*loaded_data[0].shape[0]\n",
    "    test_data = [np.split(data,splits.astype(int))[2] for data in loaded_data]\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetContactForce\n",
    "from Encoding import ConvertToDirected\n",
    "data = Rollout.GroundTruth[0].clone()\n",
    "force = GetContactForce(data)\n",
    "GT = np.loadtxt('PairContact001.txt')\n",
    "\n",
    "data =ConvertToDirected(data)\n",
    "force2 = GetContactForce(data)\n",
    "\n",
    "print(torch.norm(force,dim=1).size())\n",
    "print(torch.norm(force2,dim=1))\n",
    "\n",
    "torch.all(np.round(GT[:,2],2)==torch.round(torch.norm(force2,dim=1),decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import GetInternalStressRollout\n",
    "stress = GetInternalStressRollout(Rollout)\n",
    "torch.set_printoptions(4)\n",
    "print(\"Stress at time 0\")\n",
    "print(torch.round(stress[0],decimals=8)),\n",
    "print(\"\\nStress at time 99\")\n",
    "print(torch.round(stress[-1],decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotContactVectorAndForce, GetAllContactpoints,AxesLimits\n",
    "data = Rollout.GroundTruth[0]\n",
    "BC = Rollout.BC_rollout[3]\n",
    "fig,axs = PlotContactVectorAndForce(data,BC)\n",
    "for ax in axs: AxesLimits(ax,BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Rollout.ML_rollout[0]\n",
    "#data = Rollout.GroundTruth[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEM-GNN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
