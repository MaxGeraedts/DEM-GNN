{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM-GNN Rollout Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import json \n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from torch_geometric.nn.models import MLP\n",
    "print(torch.cuda.is_available())\n",
    "np.set_printoptions(linewidth=200)\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_functions import DEM_Dataset\n",
    "\n",
    "force_reload    = True\n",
    "dataset_name    = \"N400_Mono\"\n",
    "bundle_size     = 1\n",
    "\n",
    "pre_transform = T.Compose([T.Cartesian(False),\n",
    "                           T.Distance(norm=False,cat=True)])\n",
    "\n",
    "[dataset_train]      = [DEM_Dataset(dataset_name,\n",
    "                                                               dataset_type,\n",
    "                                                               force_reload     = force_reload,\n",
    "                                                               pre_transform    = pre_transform,\n",
    "                                                               bundle_size      = bundle_size) \n",
    "                                                               for dataset_type in [\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotTrainingLoss\n",
    "lr =1\n",
    "dataset_name=\"N400_Mono\"\n",
    "\n",
    "for lr in [\"1\",\"0.1\",\"0.001\",\"0.0001\",\"0.00001\",\"0.000001\", \"0.0000001\",\"0.00000001\"]:\n",
    "    model_ident = f\"lr{lr}\"\n",
    "    print(model_ident)\n",
    "    fig, axs = PlotTrainingLoss(dataset_name,model_ident,push=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import CompareModels, Evaluation\n",
    "Evaluation_function=Evaluation(mode='mechanics_mean',print_results=True)\n",
    "#model_idents = [\"Allout\",'Forward_Bundle','Forward','Bundle', 'Onestep']\n",
    "metrics = CompareModels(dataset_name        = 'N400_Mono',\n",
    "                        model_idents        = ['Emb16','Emb32','Emb64','Emb128','Emb256'],\n",
    "                        Evaluation_function = Evaluation_function,\n",
    "                        save_name           = \"Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = CompareModels(dataset_name        = '2Sphere',\n",
    "                        model_idents        = ['Allout'],\n",
    "                        Evaluation_function = Evaluation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trained model\n"
     ]
    }
   ],
   "source": [
    "from Plotting import PlotXYZ, PlotStressComparison\n",
    "from ML_functions import LearnedSimulator, NormalizeData, GetModel, Rescale, NormalizePos, MaskTestData\n",
    "from Encoding import SaveRolloutAsJSON\n",
    "from Evaluation import ParticlesOutsideBoundary\n",
    "\n",
    "dataset_name=\"N400_Mono\"\n",
    "model_ident = \"dsfs\"\n",
    "\n",
    "model = GetModel(dataset_name,model_ident)[0]\n",
    "AggregatedArgs = MaskTestData(dataset_name,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 99888.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Ground Truth Rollout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 322.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Learned Rollout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:01, 83.47it/s]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Particles outside boundaries: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scale_name = f\"{dataset_name}_bund{model.bundle_size}\"\n",
    "transform = T.Compose([T.Cartesian(False),T.Distance(norm=False,cat=True),NormalizeData(dataset_name,scale_name)])\n",
    "Simulation = LearnedSimulator(model, scale_function = Rescale(dataset_name,scale_name),transform = transform)\n",
    "for sample_idx in [0]:\n",
    "    Simulation.Rollout(*AggregatedArgs,sample_idx,show_tqdm=True)\n",
    "    if dataset_name == \"2Sphere\": \n",
    "        PlotXYZ(Simulation,t_max=100, normalize=False)\n",
    "    #SaveRolloutAsJSON(Simulation.ML_rollout,dataset_name,model_ident,sample_idx)\n",
    "    print(f\"Maximum Particles outside boundaries: {np.max(ParticlesOutsideBoundary(Simulation.ML_rollout,Simulation.BC_rollout)).astype(int)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotGraphComparison\n",
    "save = False\n",
    "show = True\n",
    "for t in range(0,100,20):  \n",
    "    fig = PlotGraphComparison(t,Simulation,sample_idx,Simulation.tol,plot_lines=True)\n",
    "    if save == True: plt.savefig(f\"{os.getcwd()}\\\\Figures\\\\Plots\\\\Graph_Sample{sample_idx}_Time{t}_Tol{str(Simulation.tol)[2:]}.png\",bbox_inches='tight')     \n",
    "    if show == True: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotFres\n",
    "from Evaluation import NormalizedResultantForce\n",
    "list = [NormalizedResultantForce(data) for data in Simulation.GroundTruth]\n",
    "Fnorm_ML = np.array([NormalizedResultantForce(data) for data in Simulation.ML_rollout])\n",
    "Fnorm_GT = np.array([NormalizedResultantForce(data) for data in Simulation.GroundTruth])\n",
    "fig = PlotFres(np.mean(Fnorm_GT,axis=1),np.mean(Fnorm_ML,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotStressComparison\n",
    "PlotStressComparison(Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotForceDistributionComparison\n",
    "quantiles = [5,10,25,50]\n",
    "fig, ax = PlotForceDistributionComparison(Fnorm_GT,Fnorm_ML,quantiles,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "from Plotting import MakeGIF, PlotMeshNormals\n",
    "\n",
    "gifname = f\"{dataset_name}_GT_deform\"\n",
    "gifname = f\"{dataset_name}_{model_ident}_deform\"\n",
    "\n",
    "datalist = Simulation.GroundTruth\n",
    "datalist = Simulation.ML_rollout\n",
    "MakeGIF(datalist,gifname,fps=8,color='lightblue',deformation=True)\n",
    "\n",
    "#data = Rollout.ML_rollout[10]\n",
    "#data = Rollout.GroundTruth[0]\n",
    "#PlotMeshNormals(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking topology functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ConstructTopology\n",
    "from ML_functions import MaskTestData\n",
    "dataset_name = 'N400_Mono'\n",
    "AggregatedArgs = MaskTestData(dataset_name,'test')\n",
    "data,top,bc = AggregatedArgs\n",
    "data = data[0]\n",
    "top = top[0]\n",
    "bc = bc[0]\n",
    "t=0\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "\n",
    "\n",
    "par_data = data[0]\n",
    "super_topology = ConstructTopology(par_data,BC_t,6)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,top,bc = AggregatedArgs\n",
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import TopologyFromPlausibleTopology_old, TopologyFromPlausibleTopology\n",
    "\n",
    "tol=0\n",
    "t0 = time.time()\n",
    "topology_0 = ConstructTopology(par_data,BC_t,tol)-1\n",
    "t1 = time.time()\n",
    "topology_1 = TopologyFromPlausibleTopology_old(super_topology,par_data,BC_t,tol)\n",
    "t2 = time.time()\n",
    "topology_2 = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,tol)\n",
    "t3 = time.time()\n",
    "\n",
    "print(np.all(topology_0 == topology_1))\n",
    "print(np.all(topology_1 == topology_2))\n",
    "print(t1-t0)\n",
    "print(t2-t1)\n",
    "print(t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregatedArgs = MaskTestData(dataset_name)\n",
    "data,top,bc = AggregatedArgs\n",
    "par_data = data[0][0]\n",
    "print(par_data)\n",
    "R_avg = par_data[0,3]\n",
    "noise_factor = 0.01\n",
    "standard_deviation = noise_factor*R_avg\n",
    "noise = np.array(standard_deviation*torch.randn((par_data.shape[0],3)))\n",
    "par_data[:,:3]+=noise\n",
    "print(par_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=40\n",
    "BC_t = bc.copy()\n",
    "BC_t[:,:3] = bc[:,:3]+(t+1)*bc[:,-3:]\n",
    "bc=BC_t\n",
    "par_data = data[t]\n",
    "top0 = ConstructTopology(par_data,bc,0)-1\n",
    "\n",
    "from Encoding import TopologyFromPlausibleTopology\n",
    "topology_sub = TopologyFromPlausibleTopology(super_topology,par_data,BC_t,0)\n",
    "\n",
    "topology_sub == top0,topology_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetGamma\n",
    "gamma = GetGamma(data)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageDX(data):\n",
    "    norm = torch.norm(data.y,dim=1)\n",
    "    return torch.mean(norm)\n",
    "\n",
    "dataset_test.y.abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDatasetStats(dataset):\n",
    "    torch.set_printoptions(sci_mode=False, linewidth=150)\n",
    "    print(f\"x_max:      {dataset.x.max(dim=0)[0]}\")\n",
    "    print(f\"y_mean:     {dataset.y.mean(dim=0)}\")\n",
    "    print(f\"y_std:      {dataset.y.std(dim=0)}\")\n",
    "    print(f\"edge_mean:  {dataset.edge_attr.mean(dim=0)}\")\n",
    "    print(f\"edge_std:   {dataset.edge_attr.std(dim=0)}\")\n",
    "    print(\"\\n\")\n",
    "    torch.set_printoptions(profile='default')\n",
    "\n",
    "PrintDatasetStats(dataset_train)\n",
    "#PrintDatasetStats(dataset_train_push)\n",
    "\n",
    "fig = plt.hist(dataset_train_push.push_forward_steps,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterStart(dataset):\n",
    "    idx = np.nonzero([data.time == 0 for data in dataset])\n",
    "    return torch.utils.data.Subset(dataset_test,idx)[0]\n",
    "\n",
    "#dataset_test_start = FilterStart(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import ToPytorchData, GetLength\n",
    "\n",
    "def GetLimits(data):\n",
    "    max = [torch.max(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    min = [torch.min(data.x[:,i]) for i in [0, 1, 2]]\n",
    "    max = torch.stack(max)\n",
    "    min = torch.stack(min)\n",
    "    limits = torch.stack([min,max],dim=1)\n",
    "    return limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding import load\n",
    "def SplitData(dataset_name,data_split):\n",
    "    loaded_data = load(dataset_name)\n",
    "    splits=np.array(data_split)*loaded_data[0].shape[0]\n",
    "    test_data = [np.split(data,splits.astype(int))[2] for data in loaded_data]\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import GetContactForce\n",
    "from Encoding import ConvertToDirected\n",
    "data = Rollout.GroundTruth[0].clone()\n",
    "force = GetContactForce(data)\n",
    "GT = np.loadtxt('PairContact001.txt')\n",
    "\n",
    "data =ConvertToDirected(data)\n",
    "force2 = GetContactForce(data)\n",
    "\n",
    "print(torch.norm(force,dim=1).size())\n",
    "print(torch.norm(force2,dim=1))\n",
    "\n",
    "torch.all(np.round(GT[:,2],2)==torch.round(torch.norm(force2,dim=1),decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import GetInternalStressRollout\n",
    "stress = GetInternalStressRollout(Rollout)\n",
    "torch.set_printoptions(4)\n",
    "print(\"Stress at time 0\")\n",
    "print(torch.round(stress[0],decimals=8)),\n",
    "print(\"\\nStress at time 99\")\n",
    "print(torch.round(stress[-1],decimals=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plotting import PlotContactVectorAndForce, GetAllContactpoints,AxesLimits\n",
    "data = Rollout.GroundTruth[0]\n",
    "BC = Rollout.BC_rollout[3]\n",
    "fig,axs = PlotContactVectorAndForce(data,BC)\n",
    "for ax in axs: AxesLimits(ax,BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformMLP(torch.nn.Sequential):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_hidden_layers,activation_function=torch.nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_hidden_layers = num_hidden_layers,\n",
    "        self.activation_function=activation_function\n",
    "        hidden_input_size = input_size\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.add_module(f\"dense{i}\",torch.nn.Linear(hidden_input_size,hidden_size))\n",
    "            self.add_module(f\"act{i}\",activation_function)\n",
    "            hidden_input_size = hidden_size\n",
    "        self.add_module(f\"output\",torch.nn.Linear(hidden_size,output_size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEM-GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
